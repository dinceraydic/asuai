{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2c86553-ee05-4811-9321-52ff01aba9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import streamlit as st\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f36973d-6245-40cc-8b9b-726aa59a32b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e615f775-bbde-4e39-9c86-3880017e7853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade --force-reinstall llama-cpp-python --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77ce8a19-bbf5-4dee-bce4-3eb450f081d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import LlamaCpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f7c9ec4-85af-48c8-b57f-24135a17294e",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model_path = r\"D:\\_programlama\\ML\\LangChainTutorial_1\\models\\chat\\llama-2-7b-chat.Q3_K_M.gguf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf70425-36ff-4a55-875a-2b419a819ff2",
   "metadata": {},
   "source": [
    "# Load and split document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0687f751-1846-489e-b788-f735092b2254",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d865a297-6d54-435a-95fb-5058f6630c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_size = 250\n",
    "vector_store_persist_directory = f\"./vector-store/asu_ai_db_tr_{token_size}_llama_en\"\n",
    "original_document_path = \"./data/yeni_sss.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3aa5211-0ea1-43da-9da3-f18087be46cc",
   "metadata": {},
   "source": [
    "## Embedding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1b6570f-833d-46a4-bbe8-cc53e8d2b020",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import LlamaCppEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6244b338-726c-46fe-987c-89b22698dcaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "embedding_function = LlamaCppEmbeddings(model_path=my_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4598eca7-4cd6-4e7e-8d35-ea6f6b4fb1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(vector_store_persist_directory):\n",
    "    # Load documents\n",
    "    loader = TextLoader(original_document_path, encoding=\"UTF-8\")\n",
    "    documents = loader.load()\n",
    "    text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=token_size)\n",
    "\n",
    "    # Split and persist documents\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "    db = Chroma.from_documents(\n",
    "        docs, embedding_function, persist_directory=vector_store_persist_directory\n",
    "    )\n",
    "    db.persist()\n",
    "else:\n",
    "    db = Chroma(persist_directory=vector_store_persist_directory, embedding_function=embedding_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8aa9241-5dbc-403f-9365-75444b0d019e",
   "metadata": {},
   "source": [
    "## Callbacks support token-wise streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39e1453c-ae3e-41cf-a790-f3b8d2f17fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d678fd-ed55-479c-8507-4f9db8ad8554",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c0789c7-8ad1-4d46-89c4-5984cb603bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import HumanMessagePromptTemplate, SystemMessagePromptTemplate, ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cda10fb5-0728-44f6-a63f-3a8252fee184",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "n_gpu_layers = 40  # Change this value based on your model and your GPU VRAM pool.\n",
    "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=my_model_path,\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,  # Verbose is required to pass to the callback manager\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af7134d3-c553-4dff-881e-96c4c8a8ca29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1244697-f510-4fcc-810d-420473f19792",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=db.as_retriever(),\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "def answer_my_question(question):\n",
    "    unique_docs = retriever_from_llm.get_relevant_documents(query=question)\n",
    "\n",
    "    template = \"\"\"{question} sorusuna aşağıda verilen bağlamdaki bilgilere ugun şekilde cevap ver.\\n{baglam}.\n",
    "    Cevapta sadece sorunun en kısa cevabını döndür. Soru yeterince açık değilse soru alternatifleri vererek tekrar sorulmasını iste.\"\"\"\n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"question\",\"baglam\"])\n",
    "\n",
    "   \n",
    "    llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "    llm_chain.run(question=question, baglam=unique_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2594ca5-3911-43bc-8b1b-7697f328cc61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Version 1: What master's programs have zero cost and are available in real-time?\n",
      "Version 2: Can you provide a list of fully funded master's degrees that I can access immediately?\n",
      "Version 3: How do I find graduate programs with no tuition fees and immediate availability?"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Which master's programs are free?\n",
      "\n",
      "The following master's programs are free:\n",
      "\n",
      "1. Master of Arts in Teaching (MAT) - This program is offered by the University of Georgia and is designed for students who want to become teachers in grades 4-12. The program is fully online and does not charge tuition.\n",
      "2. Master of Science in Environmental Science (MSES) - This program is offered by the University of Denver and focuses on environmental science, policy, and management. The program is completely free, thanks to a grant from the National Science Foundation.\n",
      "3. Master of Public Health (MPH) - Many universities offer MPH programs that are free or low-cost, including the University of California, Berkeley, and the University of Michigan. These programs are designed for students who want to pursue careers in public health, epidemiology, and health policy.\n",
      "4. Master of Business Administration (MBA) - Some universities offer MBA programs that are free or low-cost, including the Massachusetts Institute of Technology (MIT) and the University of California, Berkeley. These programs are designed for students who want to pursue careers"
     ]
    }
   ],
   "source": [
    "answer_my_question(\"Which master's programs are free?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82c8512-6f0c-4798-b94d-521b3405de88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d965ba83-e888-468c-b5b5-1839b3a2ab12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ea8901-e6ce-489c-b983-16b7a3e0a236",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5243d263-acfa-48ff-888f-71a7f406403c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Callbacks support token-wise streaming\n",
    "# callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9c0e162-dd62-4099-ae2b-b74ac611b42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# template = \"\"\"Question: {question}\n",
    "\n",
    "# Answer: Let's work this out in a step by step way to be sure we have the right answer.\"\"\"\n",
    "\n",
    "# prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343da3fc-c1b6-4bae-a0ca-f86b98c8d825",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a78ea80-5af8-4f31-98d9-06bd016e82f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "# question = \"What NFL team won the Super Bowl in the year Justin Bieber was born?\"\n",
    "# llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d298861-c8e7-4303-8067-3db296a92c5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
